{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.network import Network\n",
    "from src.classes.node import Node\n",
    "from src.experimentation import generate_networks, read_and_load_networks, multiple_correlations_par\n",
    "from multiprocessing import Pool\n",
    "from src.viusalization import plot_cascade_animation, test_significance, calculate_average_per_gamma, plot_cascades_gamma #,plot_cascade_dist_average\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from collections import defaultdict\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = np.linspace(-1, 1, 11)\n",
    "correlations = np.round(correlations, 1)\n",
    "initial_seeds = np.linspace(13, 1600, 11)\n",
    "num_runs = 30\n",
    "num_nodes = 200\n",
    "update_fraction = 0.1\n",
    "average_degree = 8\n",
    "starting_distribution = 0.5     # L / R ratio (niet per se nodig maar kan misschien leuk zijn om te varieern)\n",
    "p = average_degree/(num_nodes-1) \n",
    "updates = 300000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting seed for fixed order for sets, for reproducability purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env PYTHONHASHSEED=134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def multiple_correlations(corr, start_seed = 39):\n",
    "\n",
    "#     num_nodes = 200\n",
    "#     correlation = corr\n",
    "#     update_fraction = 0.1\n",
    "#     average_degree = 8\n",
    "#     starting_distribution = 0.5     # L / R ratio (niet per se nodig maar kan misschien leuk zijn om te varieern)\n",
    "\n",
    "#     # average degree of 8\n",
    "#     p = average_degree/(num_nodes-1)\n",
    "#     seedje = start_seed\n",
    "#     number_of_experiments = 10\n",
    "#     collection_of_all_before = defaultdict(list)\n",
    "#     collection_of_all_after = defaultdict(list)\n",
    "#     largest_size_of_all = 0\n",
    "#     save=True\n",
    "\n",
    "\n",
    "#     def develop_network(num_nodes, correlation, update_fraction, starting_distribution, seedje, p):\n",
    "#         network = Network(num_nodes, mean=0, correlation=correlation, update_fraction=update_fraction, starting_distribution=starting_distribution, seed=seedje, p=p)\n",
    "\n",
    "#         number_of_iters=10000\n",
    "#         data_before, average_data_before = create_data(number_of_iters, network)\n",
    "#         largest_size = max(data_before.keys())\n",
    "\n",
    "#         number_of_alterations = 0\n",
    "\n",
    "        \n",
    "#         for _ in range(1000000):\n",
    "#             network.update_round()\n",
    "#             number_of_alterations += network.alterations\n",
    "#         print(number_of_alterations)\n",
    "\n",
    "#         after_data, average_after_data = create_data(number_of_iters, network)\n",
    "#         if max(after_data.keys()) > largest_size:\n",
    "#             largest_size = max(after_data.keys())\n",
    "#         return (data_before, average_data_before), (after_data, average_after_data), largest_size\n",
    "\n",
    "#     for i in range(number_of_experiments):\n",
    "\n",
    "#         seedje +=i\n",
    "#         (before_data, averaged_before_data), (after_data, averaged_after_data), largest_size=develop_network(num_nodes, correlation, update_fraction, starting_distribution, seedje, p)\n",
    "#         if largest_size > largest_size_of_all:\n",
    "#             largest_size_of_all = largest_size\n",
    "#         for size, polarizations in before_data.items():\n",
    "#             collection_of_all_before[size].extend(polarizations)\n",
    "\n",
    "#         for size, polarizations in after_data.items():\n",
    "#             collection_of_all_after[size].extend(polarizations)\n",
    "        \n",
    "\n",
    "#     plot_cascade_dist_average(collection_of_all_before, \"before\", largest_size_of_all, number_of_experiments, save, correlation)\n",
    "#     plot_cascade_dist_average(collection_of_all_after, \"after\", largest_size_of_all, number_of_experiments, save, correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developing and saving network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # dummy values\n",
    "# num_runs = 10\n",
    "# updates=10000\n",
    "\n",
    "# generate_networks(correlations, initial_seeds, num_nodes=num_nodes, iterations=updates, how_many=num_runs, update_fraction=update_fraction, starting_distribution=starting_distribution, p=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Reading in and generating Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_networks = read_and_load_networks(num_runs, num_nodes, update_fraction, average_degree, starting_distribution, correlations)\n",
    "\n",
    "\n",
    "# test = False\n",
    "\n",
    "# if test:\n",
    "#     # Test consistency of the networks\n",
    "#     used_seed = int(initial_seeds[0])\n",
    "#     test_network = Network(\"random\", num_nodes, mean=0, correlation=-1.0, update_fraction=update_fraction, starting_distribution=starting_distribution, seed=used_seed, p=p)\n",
    "#     number_of_alterations = 0\n",
    "#     for _ in range(updates):\n",
    "#         test_network.update_round()\n",
    "#         number_of_alterations += test_network.alterations\n",
    "#         test_network.clean_network()\n",
    "        \n",
    "#     assert set([(conn[0].ID, conn[1].ID) for conn in all_networks[(-1.0, 0)][1].connections]) == set([(conn[0].ID, conn[1].ID) for conn in test_network.connections]), \"The networks that are generated should be the same\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing and plotting the Cascade distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_data(iters, network):\n",
    "\n",
    "#     all_cascade_sizes = []\n",
    "#     all_polarizations = []\n",
    "#     average_cascade_per_round = []\n",
    "#     average_polarization_per_round = []\n",
    "#     number_of_samplers = 20\n",
    "\n",
    "#     for _ in range(iters): \n",
    "#         cascades, cascade_dist, cascade_polarization = network.analyze_network()\n",
    "#         average_cascade_per_round.append(sum(cascade_dist)/number_of_samplers)\n",
    "#         average_polarization_per_round.append(sum(cascade_polarization))\n",
    "#         all_cascade_sizes += cascade_dist\n",
    "#         all_polarizations += cascade_polarization\n",
    "\n",
    "#         # plot_network(network, cascades)\n",
    "\n",
    "#     data = defaultdict(list)\n",
    "#     for i, (size, polarization) in enumerate(zip(all_cascade_sizes, all_polarizations), 1):\n",
    "#         data[size].append(polarization)\n",
    "#     for size in data:\n",
    "#         data[size].sort()\n",
    "\n",
    "#     average_data = defaultdict(list)\n",
    "#     for (size, polarization) in zip(average_cascade_per_round, average_polarization_per_round):\n",
    "#         average_data[size].append(polarization) \n",
    "#     for size in average_data: \n",
    "#         average_data[size].sort()\n",
    "        \n",
    "#     return data, average_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the cascades (parallelized implementation)\n",
    "### Reading in network and saving everything in datastructures. structured by size and correlation value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascades_before = defaultdict(lambda: defaultdict(list))\n",
    "cascades_after = defaultdict(lambda: defaultdict(list))\n",
    "cascades_before_averaged = defaultdict(lambda: defaultdict(list))\n",
    "cascades_after_averaged = defaultdict(lambda: defaultdict(list))\n",
    "save=True\n",
    "sizes = defaultdict()\n",
    "sizes_averaged = defaultdict()\n",
    "\n",
    "\n",
    "for corr in correlations: \n",
    "    print(f\"starting experimentation for correlation: {corr}\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    (before_after, before_after_averaged, largest_sizes) = multiple_correlations_par(corr, num_runs, num_nodes, update_fraction, average_degree, starting_distribution)\n",
    "    (collection_of_all_before, collection_of_all_after) = before_after\n",
    "    (coll_of_all_before_averaged, coll_of_all_after_averaged) = before_after_averaged\n",
    "    (largest_size_of_all, largest_size_of_all_averaged) = largest_sizes\n",
    "    # plot_cascade_dist_average(collection_of_all_before, \"before\", largest_size_of_all, num_runs, save, corr)\n",
    "    # plot_cascade_dist_average(collection_of_all_after, \"after\", largest_size_of_all, num_runs, save, corr)\n",
    "    \n",
    "    sizes[corr] = largest_size_of_all\n",
    "    cascades_before[corr] = collection_of_all_before\n",
    "    cascades_after[corr] = collection_of_all_after\n",
    "\n",
    "    sizes_averaged[corr] = largest_size_of_all_averaged\n",
    "    cascades_before_averaged[corr] = coll_of_all_before_averaged\n",
    "    cascades_after_averaged[corr] = coll_of_all_after_averaged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annimation of cascade size distribution with average polarization. (average cascade size per sampled node is calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cascade_animation(cascades_before_averaged, cascades_after_averaged, list(reversed(correlations)), sizes_averaged, num_runs, save=False, averaged=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summarizing all cascade info in one plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key,value in cascades_before_averaged.items():\n",
    "#     plot_cascade_dist_average(value, \"before\", largest_size_of_all_averaged, num_runs, save, key)\n",
    "\n",
    "# for key,value in cascades_after_averaged.items():\n",
    "#     plot_cascade_dist_average(value, \"after\", largest_size_of_all_averaged, num_runs, save, key)\n",
    "\n",
    "\n",
    "plot_cascades_gamma((cascades_before_averaged, cascades_after_averaged), num_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating significance between cascade sizes and polarizations between networks before and after updating, for different gamma values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the test on your data\n",
    "values_bef, values_af, variance_bef, variance_af = calculate_average_per_gamma((cascades_before_averaged, cascades_after_averaged))\n",
    "results = test_significance(values_bef, values_af, variance_bef, variance_af, num_runs)\n",
    "\n",
    "# Print results\n",
    "for gamma, res in results.items():\n",
    "    print(f\"Gamma = {gamma}:\")\n",
    "    print(f\"  Size: t = {res['t_size']:.3f}, p = {res['p_size']:.3g}\")\n",
    "    print(f\"  Polarization: t = {res['t_pol']:.3f}, p = {res['p_pol']:.3g}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
