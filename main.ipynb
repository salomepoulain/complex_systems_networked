{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.network import Network\n",
    "from src.classes.node import Node\n",
    "from multiprocessing import Pool\n",
    "from src.viusalization import animate_network, plot_cascade_dist, plot_cascade_dist_average\n",
    "from collections import defaultdict\n",
    "import os, sys\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = np.linspace(-1, 1, 10)\n",
    "initial_seeds = np.linspace(13, 130, 10)\n",
    "num_runs = 10\n",
    "num_nodes = 200\n",
    "update_fraction = 0.1\n",
    "average_degree = 8\n",
    "starting_distribution = 0.5     # L / R ratio (niet per se nodig maar kan misschien leuk zijn om te varieern)\n",
    "p = average_degree/(num_nodes-1) \n",
    "updates = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting seed for fixed order for sets, for reproducability purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env PYTHONHASHSEED=134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def multiple_correlations(corr, start_seed = 39):\n",
    "\n",
    "#     num_nodes = 200\n",
    "#     correlation = corr\n",
    "#     update_fraction = 0.1\n",
    "#     average_degree = 8\n",
    "#     starting_distribution = 0.5     # L / R ratio (niet per se nodig maar kan misschien leuk zijn om te varieern)\n",
    "\n",
    "#     # average degree of 8\n",
    "#     p = average_degree/(num_nodes-1)\n",
    "#     seedje = start_seed\n",
    "#     number_of_experiments = 10\n",
    "#     collection_of_all_before = defaultdict(list)\n",
    "#     collection_of_all_after = defaultdict(list)\n",
    "#     largest_size_of_all = 0\n",
    "#     save=True\n",
    "\n",
    "\n",
    "#     def develop_network(num_nodes, correlation, update_fraction, starting_distribution, seedje, p):\n",
    "#         network = Network(num_nodes, mean=0, correlation=correlation, update_fraction=update_fraction, starting_distribution=starting_distribution, seed=seedje, p=p)\n",
    "\n",
    "#         number_of_iters=10000\n",
    "#         data_before, average_data_before = create_data(number_of_iters, network)\n",
    "#         largest_size = max(data_before.keys())\n",
    "\n",
    "#         number_of_alterations = 0\n",
    "\n",
    "        \n",
    "#         for _ in range(1000000):\n",
    "#             network.update_round()\n",
    "#             number_of_alterations += network.alterations\n",
    "#         print(number_of_alterations)\n",
    "\n",
    "#         after_data, average_after_data = create_data(number_of_iters, network)\n",
    "#         if max(after_data.keys()) > largest_size:\n",
    "#             largest_size = max(after_data.keys())\n",
    "#         return (data_before, average_data_before), (after_data, average_after_data), largest_size\n",
    "\n",
    "#     for i in range(number_of_experiments):\n",
    "\n",
    "#         seedje +=i\n",
    "#         (before_data, averaged_before_data), (after_data, averaged_after_data), largest_size=develop_network(num_nodes, correlation, update_fraction, starting_distribution, seedje, p)\n",
    "#         if largest_size > largest_size_of_all:\n",
    "#             largest_size_of_all = largest_size\n",
    "#         for size, polarizations in before_data.items():\n",
    "#             collection_of_all_before[size].extend(polarizations)\n",
    "\n",
    "#         for size, polarizations in after_data.items():\n",
    "#             collection_of_all_after[size].extend(polarizations)\n",
    "        \n",
    "\n",
    "#     plot_cascade_dist_average(collection_of_all_before, \"before\", largest_size_of_all, number_of_experiments, save, correlation)\n",
    "#     plot_cascade_dist_average(collection_of_all_after, \"after\", largest_size_of_all, number_of_experiments, save, correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developing and saving network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_properties(network, seed):\n",
    "    # Replace with actual calculations for your network\n",
    "    corr = network.correlation\n",
    "    node_info = []\n",
    "    connection_IDs = []\n",
    "    for node in network.all_nodes:\n",
    "        node_info.append((node.ID, node.identity, node.response_threshold))\n",
    "    for conn in network.connections:\n",
    "        connection_IDs.append((conn[0].ID, conn[1].ID))\n",
    "    properties = {\n",
    "        \"Number of Nodes\": len(network.all_nodes),\n",
    "        \"Number of Edges\": len(network.connections),\n",
    "        \"Correlation\": corr,\n",
    "        \"P value\": network.p,\n",
    "        \"Seed\": seed,\n",
    "        \"Update fraction\": network.update_fraction,\n",
    "        \"Connections\": connection_IDs,\n",
    "        \"Nodes\": node_info\n",
    "    }\n",
    "    return properties\n",
    "\n",
    "\n",
    "# num_runs = 10\n",
    "# correlations = np.linspace(-1, 1, 10)\n",
    "# initial_seeds = np.linspace(13, 130, 10)\n",
    "# num_nodes = 200\n",
    "# update_fraction = 0.1\n",
    "# average_degree = 8\n",
    "# starting_distribution = 0.5     # L / R ratio (niet per se nodig maar kan misschien leuk zijn om te varieern)\n",
    "# p = average_degree/(num_nodes-1) \n",
    "\n",
    "def generate_networks(correlations, initial_seeds, num_runs, iterations):\n",
    "    for j,corr in enumerate(correlations): \n",
    "        seed = int(initial_seeds[j])\n",
    "        for i in range(num_runs):\n",
    "            seed+=1\n",
    "             # average degree of 8\n",
    "            correlation = corr\n",
    "            network = Network(num_nodes, mean=0, correlation=correlation, update_fraction=update_fraction, starting_distribution=starting_distribution, seed=seed, p=p)\n",
    "\n",
    "            for stage in [\"before\",\"after\"]:\n",
    "\n",
    "                output_folder = f\"networks/random/{stage}/{correlation}\" \n",
    "                output_filename = f\"network_{i}.txt\"  \n",
    "                output_path = os.path.join(output_folder, output_filename)\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "                \n",
    "                # Get network properties\n",
    "                network_properties = get_network_properties(network, seed)\n",
    "\n",
    "                # Write the properties to the file\n",
    "                with open(output_path, \"w\") as file:\n",
    "                    file.write(\"Network Properties\\n\")\n",
    "                    file.write(\"==================\\n\")\n",
    "                    for key, value in network_properties.items():\n",
    "                        file.write(f\"{key}: {value}\\n\")\n",
    "                number_of_alterations = 0\n",
    "                if stage == \"after\":\n",
    "                    break\n",
    "                for _ in range(iterations):\n",
    "                    network.update_round()\n",
    "                    number_of_alterations += network.alterations\n",
    "                    network.clean_network()\n",
    "                print(number_of_alterations)\n",
    "\n",
    "generate_networks(correlations, initial_seeds, num_runs, updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Reading in and generating Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_network_properties(file_path):\n",
    "    \"\"\"\n",
    "    Reads network properties from a .txt file and converts them back\n",
    "    into a dictionary with appropriate datatypes.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .txt file containing network properties.\n",
    "\n",
    "    Returns:\n",
    "        dict: Network properties with restored data types.\n",
    "    \"\"\"\n",
    "    properties = {}\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    for line in lines[2:]:  # Skip the header lines\n",
    "        key, value = line.strip().split(\": \", 1)\n",
    "        if key == \"Number of Nodes\" or key == \"Number of Edges\":\n",
    "            properties[key] = int(value)\n",
    "        elif key == \"Correlation\" or key == \"P value\" or key == \"Update fraction\":\n",
    "            properties[key] = float(value)\n",
    "        elif key == \"Seed\":\n",
    "            properties[key] = int(value)\n",
    "        elif key == \"Connections\":\n",
    "            # Parse connections as a list of tuples\n",
    "            connections = eval(value)  # Use eval to safely parse the list of tuples\n",
    "            properties[key] = [(int(a), int(b)) for a, b in connections]\n",
    "        elif key == \"Nodes\":\n",
    "            # Parse nodes as a list of tuples\n",
    "            nodes = eval(value)  # Use eval to safely parse the list of tuples\n",
    "            properties[key] = [(int(node_id), identity, float(threshold)) for node_id, identity, threshold in nodes]\n",
    "        else:\n",
    "            properties[key] = value\n",
    "    return properties\n",
    "\n",
    "def read_and_load_networks(num_runs, num_nodes, update_fraction, average_degree, starting_distribution, correlations):\n",
    "    p = average_degree/(num_nodes-1) \n",
    "    networks = defaultdict(tuple)\n",
    "    for corr in correlations:\n",
    "        for i in range(num_runs):\n",
    "            network_properties = read_network_properties(f\"networks/random/after/{corr}/network_{i}.txt\")\n",
    "            seedje = network_properties[\"Seed\"]\n",
    "            search_nodes = defaultdict(Node)\n",
    "            before_network = Network(num_nodes, mean=0, correlation=corr, update_fraction=update_fraction, starting_distribution=starting_distribution, seed=seedje, p=p)\n",
    "            after_network = Network(num_nodes, mean=0, correlation=corr, update_fraction=update_fraction, starting_distribution=starting_distribution, seed=seedje, p=p)\n",
    "            after_network.connections = set()\n",
    "\n",
    "            for nodeje in after_network.all_nodes:\n",
    "                nodeje.node_connections = set()\n",
    "                search_nodes[nodeje.ID] = nodeje\n",
    "                \n",
    "            for (node1, node2) in network_properties[\"Connections\"]:\n",
    "                search_nodes[node1].node_connections.add(search_nodes[node2])\n",
    "                after_network.connections.add((search_nodes[node1], search_nodes[node2]))\n",
    "            \n",
    "            networks[(corr, i)] = (before_network, after_network)\n",
    "\n",
    "    return networks\n",
    "    \n",
    "\n",
    "all_networks = read_and_load_networks(num_runs, num_nodes, update_fraction, average_degree, starting_distribution, correlations)\n",
    "\n",
    "\n",
    "used_seed = int(initial_seeds[0] +1)\n",
    "test_network = Network(num_nodes, mean=0, correlation=-1.0, update_fraction=update_fraction, starting_distribution=starting_distribution, seed=used_seed, p=p)\n",
    "number_of_alterations = 0\n",
    "\n",
    "for _ in range(updates):\n",
    "    test_network.update_round()\n",
    "    number_of_alterations += test_network.alterations\n",
    "    test_network.clean_network()\n",
    "\n",
    "# print(number_of_alterations)\n",
    "# print([(conn[0].ID, conn[1].ID) for conn in all_networks[(-1.0, 0)][1].connections])\n",
    "# print([(conn[0].ID, conn[1].ID) for conn in test_network.connections])\n",
    "assert set([(conn[0].ID, conn[1].ID) for conn in all_networks[(-1.0, 0)][1].connections]) == set([(conn[0].ID, conn[1].ID) for conn in test_network.connections]), \"The networks that are generated should be the same\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing and plotting the Cascade distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(iters, network):\n",
    "\n",
    "    all_cascade_sizes = []\n",
    "    all_polarizations = []\n",
    "    average_cascade_per_round = []\n",
    "    average_polarization_per_round = []\n",
    "    number_of_samplers = 20\n",
    "\n",
    "    for _ in range(iters): \n",
    "        cascades, cascade_dist, cascade_polarization = network.analyze_network()\n",
    "        average_cascade_per_round.append(sum(cascade_dist)/number_of_samplers)\n",
    "        average_polarization_per_round.append(sum(cascade_polarization))\n",
    "        all_cascade_sizes += cascade_dist\n",
    "        all_polarizations += cascade_polarization\n",
    "\n",
    "        # plot_network(network, cascades)\n",
    "\n",
    "    data = defaultdict(list)\n",
    "    for i, (size, polarization) in enumerate(zip(all_cascade_sizes, all_polarizations), 1):\n",
    "        data[size].append(polarization)\n",
    "    for size in data:\n",
    "        data[size].sort()\n",
    "\n",
    "    average_data = defaultdict(list)\n",
    "    for (size, polarization) in zip(average_cascade_per_round, average_polarization_per_round):\n",
    "        average_data[size].append(polarization) \n",
    "    for size in average_data: \n",
    "        average_data[size].sort()\n",
    "        \n",
    "    return data, average_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_correlations(corr,all_networks):\n",
    "\n",
    "    number_of_experiments = 10\n",
    "    number_of_iters = 10000\n",
    "    collection_of_all_before = defaultdict(list)\n",
    "    collection_of_all_after = defaultdict(list)\n",
    "    largest_size_of_all = 0\n",
    "    save=False\n",
    "    \n",
    "    # return (data_before, average_data_before), (after_data, average_after_data), largest_size\n",
    "\n",
    "    for i in range(number_of_experiments):\n",
    "\n",
    "        before_network, after_network = all_networks[(corr, i)]\n",
    "    \n",
    "        before_data, average_before_data = create_data(number_of_iters, before_network)\n",
    "        after_data, average_after_data = create_data(number_of_iters, after_network)\n",
    "\n",
    "        largest_size = max(before_data.keys())\n",
    "        if max(after_data.keys()) > largest_size:\n",
    "            largest_size = max(after_data.keys())\n",
    "\n",
    "        if largest_size > largest_size_of_all:\n",
    "            largest_size_of_all = largest_size\n",
    "        for size, polarizations in before_data.items():\n",
    "            collection_of_all_before[size].extend(polarizations)\n",
    "\n",
    "        for size, polarizations in after_data.items():\n",
    "            collection_of_all_after[size].extend(polarizations)\n",
    "        \n",
    "        print(f\"finsihed cascade experimentation {i}\")\n",
    "        \n",
    "\n",
    "    plot_cascade_dist_average(collection_of_all_before, \"before\", largest_size_of_all, number_of_experiments, save, corr)\n",
    "    plot_cascade_dist_average(collection_of_all_after, \"after\", largest_size_of_all, number_of_experiments, save, corr)\n",
    "\n",
    "    return collection_of_all_before, collection_of_all_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascades_before = defaultdict(lambda: defaultdict(list))\n",
    "cascades_after = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for corr in correlations: \n",
    "    print(f\"starting experimentation for correlation: {corr}\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    # initial_seed = int(initial_seeds[i])\n",
    "    cs_before, cs_after = multiple_correlations(corr,all_networks)\n",
    "    cascades_before[corr] = cs_before\n",
    "    cascades_after[corr] = cs_after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.network import Network\n",
    "from src.viusalization import animate_network\n",
    "\n",
    "def create_network():\n",
    "    num_nodes = 200\n",
    "    correlation = -1\n",
    "    update_fraction = 0.3\n",
    "    starting_distribution = 0.5     # L / R ratio (niet per se nodig maar kan misschien leuk zijn om te varieern)\n",
    "    p = 0.01\n",
    "    k = 2\n",
    "\n",
    "\n",
    "    netwerkje = Network(num_nodes, correlation, update_fraction, starting_distribution, seed=32, p=p)\n",
    "    animation = animate_network(netwerkje)\n",
    "create_network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experimentation import experiment_a\n",
    "\n",
    "pars = []\n",
    "seed = 33\n",
    "for i in range(10):\n",
    "    seed += i*10\n",
    "    netwerkje = Network(100, -1, p=0.02, seed=seed)\n",
    "    pars.append(netwerkje)\n",
    "    \n",
    "PROCESSES = 10\n",
    "assert PROCESSES < os.cpu_count(), \"Lower the number of processes (PROCESSES)\"\n",
    "with Pool(PROCESSES) as pool:\n",
    "        assert PROCESSES < os.cpu_count(), \"Lower the number of processes (PROCESSES)\"\n",
    "        print(f\"Starting parallel execution for {experiment_a} schedule\")\n",
    "        results = pool.map(experiment_a, pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
